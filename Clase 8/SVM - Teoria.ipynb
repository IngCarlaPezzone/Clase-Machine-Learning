{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb220121-74bd-41dd-9afa-554fb609058d",
   "metadata": {},
   "source": [
    "# Algoritmo Maquinas Vectores Soporte (Support Vector Machine - SVM)\n",
    "\n",
    "Son un conjunto de algoritmos de aprendizaje supervisado. Se aplican a problemas de clasificación y regresión. Dado un conjunto de ejemplos de entrenamiento podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra.\n",
    "\n",
    "## SVM para Clasificación\n",
    "\n",
    "El SVM construye un hiperplano en un espacio multidimensional para separar las diferentes clases. El SVM genera un hiperplano óptimo de forma iterativa, que se utiliza para minimizar un error. La idea central de SVM es encontrar un hiperplano marginal máximo que mejor divida el conjunto de datos en clases.\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/13.png\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "Definamos cada uno de los elementos que compone esta figura:\n",
    "\n",
    "- **Vectores de Soporte**: son los puntos de datos más cercanos al hiperplano. Estos puntos definirán mejor la línea de separación calculando los márgenes. Estos puntos son más relevantes para la construcción del clasificador.\n",
    "\n",
    "- **Hiperplano**: es un plano de decisión que separa entre un conjunto de objetos que tienen membresía de clase diferentes.\n",
    "\n",
    "- **Margen**: es un espacio entre las dos líneas en los puntos más cercanos de la clase. Se calcula como la distancia perpendicular desde la línea hasta los vectores de soporte o puntos más cercanos. Si el margen es mayor entre las clases, entonces se considera un buen margen, un margen menor es un mal margen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ee154-2ad4-41bf-806d-c14973f9704f",
   "metadata": {},
   "source": [
    "### ¿Cómo función el algoritmo de Máquinas de Soporte de Vectores?\n",
    "\n",
    "El **objetivo** principal es segregar el conjunto de datos de la mejor manera posible. La distancia entre los puntos más cercanos se conoce como el margen. El objetivo es seleccionar un hiperplano con el máximo margen posible entre vectores de soporte en el conjunto de datos dado.\n",
    "\n",
    "Para buscar el mejor hiperplano se siguen los siguientes pasos:\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/14.png\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "1. Generar hiperplanos que segreguen las clases de la mejor manera. Como podemos observar en la figura tenemos tres hiperplanos que separan los datos, pero solamente uno de ellos está separando las dos clases correctamente, el resto tienen mayor error de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab7bb5-6f5b-4a0f-9e46-b37e14a176ba",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/15.png\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "2. Seleccionar el hiperplano correcto con la máxima segregación de los puntos de datos más cercanos, como se muestra en la figura.\n",
    "\n",
    "En el ejemplo mostrado acá se puede resolver con un hiperplano lineal, pero no siempre esto se puede resolver tan fácilmente. En tales situaciones, el algoritmo utiliza un truco del kernel para transformar el espacio de entrada en un espacio dimensional superior, como se muestra en la figura.\n",
    "   \n",
    "<figure>\n",
    "<img align=\"right\", src=\"./Imagenes/16.png\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "   Los puntos de datos se grafican en los ejes $x$ y $z$, en donde $z$ es la suma cuadrada de $x$ y $y$ ($z = x^2 + y^2$). Ahora se puede separar fácilmente estos puntos utilizando la separación lineal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab758f2b-36f7-4989-904d-33445a89571d",
   "metadata": {},
   "source": [
    "## SVM para Regresión\n",
    "\n",
    "El algoritmo  de Vectores de Soporte Regresión se basa en buscar la curva o hiperplano que modele la tendencia de los datos de entrenamiento y según ella predecir cualquier dato en el futuro.\n",
    "\n",
    "<figure>\n",
    "<img align=\"center\", src=\"./Imagenes/17.png\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "Se basa en predecir valores numéricos, dado que la salida es un número real, se vuelve muy difícil predecir la información disponible, que tiene infinitas posibilidades, sin embargo, la idea principal es siempre la misma: minimizar el error, individualizar el hiperplano que maximiza el margen, teniendo en cuenta que se tolera parte del error.\n",
    "\n",
    "El hiperplano que se obtiene dentro de este algoritmo siempre tratará de moldear el comportamiento de los datos y esta curva siempre viene acompañada con un rango (máximo margen), tanto del lado positivo como en el negativo, el cual tiene el mismo comportamiento o forma de la curva.\n",
    "\n",
    "Todos los datos que se encuentren fuera del rango son considerados errores por lo que es necesario calcular la distancia entre el mismo y los rangos. Esta distancia lleva por nombre *epsilon* y afecta la ecuación final del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29930c52-36a2-4f1f-a33b-3f483b41e010",
   "metadata": {},
   "source": [
    "### ¿Cómo se construye?\n",
    "\n",
    "Este algoritmo funciona muy bien para datos lineales como no lineales, pero realicemos la explicación con datos lineales para que sea más fácil de entender. Entonces expliquemos paso a paso cómo se construye este algoritmo.\n",
    "\n",
    "<figure>\n",
    "<img align=\"right\", src=\"./Imagenes/18.png\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "1. Tenemos el siguiente conjunto de datos, que, como podemos observar son datos lineales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cc077-1839-43c1-81d1-23e4c20fbcf0",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img align=\"right\", src=\"./Imagenes/19.png\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "2. Lo primero que debemos realizar es obtener un hiperplano que mejor represente el comportamiento de los datos, como el ejemplo que estamos usando son datos lineales este hiperplano es simplemente una línea, pero cuando se trabaja con datos no lineales el hiperplano es mucho más complicado a este. La fórmula para este hiperplano será la misma a la de una línea:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292c1ce-6692-49e0-a804-1242070cd2dc",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img align=\"right\", src=\"./Imagenes/20.png\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "3. El siguiente paso es construir unas bandas paralelas al hiperplano que cubra la mayor cantidad de datos, a estas bandas se le conoce como *vectores de apoyo o de soporte*.\n",
    "\n",
    "   Ahora bien, como podemos observar estas bandas no cubrieron todos los datos, todavía tenemos puntos fuera de la misma, estos datos serían los errores y los que se deben considerar para la fórmula del algoritmo. Acá lo que se calcula es la distancia entre las bandas y el punto, a esta distancia se le da el nombre de *epsilon*.\n",
    "\n",
    "   Al final, la fórmula completa para el cálculo de este algoritmo, utilizando datos lineales es la siguiente:\n",
    "   \n",
    "<figure>\n",
    "<img align=\"center\", src=\"./Imagenes/21.png\"   style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "En donde:\n",
    "\n",
    "- $w$ es la magnitud del vector o hiperplano\n",
    "- $C$ es una constante y debe ser mayor a 0, determina el equilibrio entre la regularidad de la función y la cuantía hasta la cual toleramos desviaciones mayores que las bandas de soporte.\n",
    "- ξ y ξ* son las variables que controlan el error cometido por la función de regresión al aproximar a las bandas.\n",
    "\n",
    "> Si el valor de la constante $C$ es muy grande, en el caso límite $C$ tiende a infinito , estaríamos considerando que el conjunto está perfectamente representado por nuestro hiperplano predictor, ξ tiende a 0. Al contrario, un número demasiado pequeño para $C$ permitirá valores de ξ elevados, es decir, estaríamos admitiendo un número muy elevado de ejemplos mal representados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b3ab4-3548-42f1-995c-48d3109ff6fd",
   "metadata": {},
   "source": [
    "Por su parte para datos *no lineales* el procedimiento es exactamente igual, la diferencia es que se implementa de un Kernel para convertir los datos lineales. Acá lo importante es mostrar es que una vez llegados estos datos de forma lineal el procedimiento explicado acá es exactamente igual.\n",
    "\n",
    "<figure>\n",
    "<img align=\"center\", src=\"./Imagenes/22.png\"   style=\"width:90%;\" >\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
