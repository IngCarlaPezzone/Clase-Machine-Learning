{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e1ba74-03b6-432d-8fa5-fb0ced58e035",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427c124-b5a9-4983-bf05-edad02ed286e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ejemplo de clasificación\n",
    "\n",
    "Para explicar cómo funcionan los árboles de decisión, vamos a utilizar un ejemplo de clasificación de gatos. Estás dirigiendo un centro de adopción de gatos y, dadas unas cuantas características, quieres entrenar un clasificador que te diga rápidamente si un animal es un gato o no. Tengo aquí 10 ejemplos de entrenamiento. Asociado con cada uno de estos 10 ejemplos, vamos a tener características relativas a la forma de las orejas del animal, la forma de la cara, si tiene bigotes, y luego la etiqueta de verdad que desea predecir este gato animal. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/01.png\"   style=\"width:70%;\" >\n",
    "</figure>\n",
    "\n",
    "Este conjunto de datos contiene cinco gatos y cinco perros. Las **características de entrada x** son estas tres columnas, y la salida **objetivo** que se quiere predecir, **y**, es esta columna final de, ¿es un gato o no? \n",
    "\n",
    "En este ejemplo, las características x toman valores *categóricos*. En otras palabras, las características toman sólo unos pocos valores discretos: Sus formas son puntiagudas o blandas. La forma de la cara es redonda o no redonda y los bigotes están presentes o ausentes. \n",
    "\n",
    "Se trata de una tarea de clasificación binaria porque las etiquetas también son uno o cero. Por ahora, cada una de las características $x_1$, $x_2$ y $x_3$ toma sólo dos valores posibles. Más adelante hablaremos de las características que pueden tener más de dos valores posibles, así como de las características de valor continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b8413-14d7-4eef-ad0e-68c678ed4edd",
   "metadata": {},
   "source": [
    "## ¿Qué es un árbol de decisión?\n",
    "\n",
    "Este es un ejemplo de un modelo que puede obtenerse después de entrenar un algoritmo de aprendizaje de árbol de decisión en el conjunto de datos que acabamos de ver. \n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/02.png\"   style=\"width:100%;\" >\n",
    "</figure>\n",
    "\n",
    "- Cada uno de los óvalos o rectángulos se llama **nodo** en el árbol. \n",
    " - El nodo superior del árbol, se llama el **nodo raíz** del árbol.\n",
    " - Los nodos ovalados son **nodos de decisión** porque miran una característica particular y luego, basados en el valor de la característica, hacen que usted decida si ir a la izquierda o a la derecha en el árbol.\n",
    " - Los nodos rectangulares en la parte inferior, se llaman **nodos hoja**. Son los que hacen una predicción.\n",
    "\n",
    "La forma en que este modelo funciona es:\n",
    "\n",
    "- Tiene un nuevo ejemplo de prueba, tiene un gato donde la forma de la oreja tiene puntiaguda, la forma de la cara es redonda, y los bigotes están presentes. \n",
    "- La forma en que este modelo mirará este ejemplo y tomará una decisión de clasificación es que empezará con este ejemplo en este nodo superior del árbol (nodo raiz), y miraremos la característica escrita dentro, que es la *forma de la oreja*. \n",
    "- Basándonos en el valor de la forma de la oreja de este ejemplo iremos a la izquierda o a la derecha. El valor de la forma de la oreja de este ejemplo es puntiaguda, por lo que bajaremos por la rama izquierda del árbol, y terminaremos en este nodo ovalado siguiente. \n",
    "- Luego miramos la forma de la cara de este ejemplo, que resulta ser redonda, y entonces seguiremos esta flecha hacia abajo (nodo hoja). El algoritmo hará una inferencia que piensa que esto es un gato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd6518-b68f-4498-b18a-877a05b89e51",
   "metadata": {},
   "source": [
    "## Proceso de construcción\n",
    "\n",
    "El proceso de construcción de un árbol de decisión dado un conjunto de entrenamiento tiene unos cuantos pasos. Veamos el proceso general de lo que hay que hacer para construir un árbol de decisión, dado un conjunto de entrenamiento de 10 ejemplos de gatos y perro.\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/03.png\"   style=\"width:100%;\" >\n",
    "</figure>\n",
    "\n",
    "1. Decidir qué característica utilizar en el nodo raíz. Digamos que decidimos elegir como característica la forma de la oreja. Lo que significa es que vamos a decidir mirar todos nuestros ejemplos de entrenamiento y dividirlos según el valor de la característica de la *forma de la oreja*. \n",
    "En particular, vamos a elegir los cinco ejemplos con orejas puntiagudas y moverlos hacia abajo a la izquierda. Elijamos los cinco ejemplos con orejas caídas y desplacémoslos hacia la derecha. \n",
    "\n",
    "2. Veamos la **rama izquierda** del árbol de decisión para decidir qué nodos poner allí. En particular, qué característica queremos utilizar a continuación. Digamos que usted decide utilizar la característica de la *forma de la cara* allí. Lo que haremos ahora es tomar los cinco ejemplos del punto 1 y dividirlos en dos subconjuntos basados en su valor de la forma de la cara. Tomaremos los cuatro ejemplos de estos cinco con una forma de *cara redonda* y los moveremos hacia la izquierda. El único ejemplo con una forma de *cara no redonda* y lo moveremos hacia abajo a la derecha. \n",
    "Por último, observamos que estos cuatro ejemplos son todos gatos cuatro de ellos son gatos. En lugar de dividir más, se creó un nodo de hoja que hace una predicción. \n",
    "\n",
    "3. Repetimos un proceso similar en la **rama derecha** de este árbol de decisión. Centramos la atención en los cinco ejemplos que mencionamos en el punto 1. Tendríamos que elegir alguna característica aquí para utilizar la división de estos cinco ejemplos más, por ejemplo, la característica de los *bigotes*, entonces dividiríamos estos cinco ejemplos basados en donde los bigotes están presentes o ausentes.\n",
    "\n",
    "Notarás que uno de cada uno de los ejemplos de la izquierda son gatos y cero de cada cuatro son gatos. Cada uno de estos nodos es completamente puro, es decir, todos gatos o no gatos y ya no hay una mezcla de gatos y perros. Podemos crear estos nodos hoja, haciendo una predicción de gato a la izquierda y una predicción de no gato aquí a la derecha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303da75-18a6-48fa-b1f8-7263bc10d65c",
   "metadata": {},
   "source": [
    "## Decisiones claves durante el proceso de construcción\n",
    "\n",
    "1. **¿Cómo elegir qué características utilizar para dividir en cada nodo?** \n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/04.png\"   style=\"width:100%;\" >\n",
    "</figure>\n",
    "\n",
    "En el nodo raíz, así como en la rama izquierda y en la rama derecha del árbol de decisión, teníamos que decidir si había algunos ejemplos en ese nodo que comprendieran una mezcla de gatos y perros. Para elegir en qué característica dividir, los árboles de decisión intentan *maximizar la pureza* (o *minimizar la impureza*), es decir, llegar a los subconjuntos, que son lo más cercano posible a todos los gatos o todos los perros. Cuando los subconjuntos son así, se dicen que son subconjuntos *puros*, lo que significa que sólo hay una clase, o bien sólo gatos o bien no sólo gatos en ambas sub-ramas.\n",
    "    \n",
    "El algoritmo de aprendizaje del árbol de decisión tiene que elegir entre las distintas características ¿cuál es la que da lugar a la mayor pureza de las etiquetas en las sub-ramas izquierda y derecha? Porque si puedes llegar a un subconjunto de ejemplos altamente puro, entonces puedes predecir gato o no gato y acertar en su mayoría.\n",
    "\n",
    "2. **¿Cuándo se deja de dividir?**\n",
    "\n",
    "- El criterio que utilizamos ahora fue hasta que sé que hay 100%, todos los gatos o un 100% de los perros (no los gatos). Porque en ese punto parece natural construir un nodo hoja que sólo hace una predicción de clasificación. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/05.png\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "- Alternativamente, también podría decidir dejar de dividir cuando la división no supere la *profundidad máxima*. La profundidad de un nodo es un parámetro que se define como el número de saltos que se necesita para llegar desde el nodo raíz (que se denota la parte superior) a ese nodo en particular. Así que el nodo raíz tarda cero profundidad. Los nodos por debajo de él están en la profundidad 1 y en las de abajo estaría en la Profundidad 2. \n",
    "\n",
    "  Si usted hubiera decidido que la profundidad máxima del árbol de decisión es digamos 2, entonces decidiría no dividir ningún nodo por debajo de este nivel para que el árbol nunca llegue a la Profundidad 3. \n",
    "\n",
    "  Una de las razones por las que podría querer limitar la profundidad del árbol de decisión es:\n",
    "\n",
    "   - para asegurarse de que para nosotros el árbol no se hace demasiado grande y difícil de manejar\n",
    "   - para  mantener el árbol pequeño, lo hace menos propenso al sobreajuste. \n",
    "\n",
    "\n",
    "- Otro criterio que podría utilizar para decidir dejar de dividir podría ser cuando mejora en la *puntuación de prioridad*, es decir, de cuando se encuentra por debajo de un determinado umbral. Esto es, si al dividir un nodo se obtienen mejoras mínimas en la pureza o disminuye la impureza. Pero si las ganancias son demasiado pequeñas, podrías no molestarte. \n",
    "  \n",
    "  De nuevo, esto es tanto para mantener los árboles más pequeños como para reducir el riesgo de sobreajuste.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/06.png\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "- Por último, si el *número de ejemplos* de un nodo está por debajo de un determinado umbral, entonces también podría decidir dejar de dividir. Por ejemplo, si en el nodo raíz hemos dividido en la característica de la forma de la cara, entonces la rama derecha habrá tenido sólo tres ejemplos de entrenamiento con un gato y dos perros y en lugar de dividir esto en subconjuntos aún más pequeños, si usted decidió no dividir más, entonces usted acaba de crear un nodo de decisión y porque esto hace una predicción de no gato. \n",
    "\n",
    "  De nuevo, una razón por la que podrías decidir que no vale la pena dividir esto es para mantener el árbol más pequeño y evitar el sobreajuste.\n",
    "\n",
    "\n",
    "> Andrew Ng dice: *Cuando miro las tareas de aprendizaje de los árboles de decisión, a veces me siento como chico, hay un montón de piezas diferentes y un montón de cosas diferentes en este algoritmo.\n",
    "> Parte de la razón por la que podría sentirse así está en la evolución de los árboles de decisión. Hubo un investigador que propuso una versión básica de los árboles de decisión y luego un investigador diferente dijo, oh, podemos modificar esta cosa de esta manera, como su nuevo criterio de división. Luego, otro investigador propuso una cosa diferente como, oh, tal vez deberíamos dejar de sudar cuando se alcanza una cierta profundidad máxima. A lo largo de los años, diferentes investigadores vinieron con diferentes refinamientos del algoritmo.\n",
    "> Como resultado de ello, funciona muy bien, pero cuando nos fijamos en todos los detalles de cómo implementar un árbol de decisión, se siente un montón de piezas diferentes como ¿por qué hay tantas maneras diferentes de decidir cuándo dejar de dividir?. \n",
    "> Si a usted le parece algo complicado y desordenado, a mí también me lo parece. Pero estas piezas diferentes, encajan en un algoritmo de aprendizaje muy eficaz y lo que se tiene que aprender es la clave, las ideas más importantes para que funcione bien.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e7ffa-3459-4690-bd1b-5a1c36494c26",
   "metadata": {},
   "source": [
    "## ¿Cómo se mide la pureza?\n",
    " \n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/07.png\"   style=\"width:80%;\" >\n",
    "</figure>\n",
    "\n",
    "Dado un conjunto de seis ejemplos como éste, tenemos tres gatos y tres perros, definamos $p_1$ como la fracción de ejemplos que son gatos, es decir, la *fracción de ejemplos con etiqueta 1*, que es lo que indica el subíndice uno. $p_1$ en este ejemplo es igual a 3/6. \n",
    "\n",
    "Vamos a *medir la impureza de un conjunto de ejemplos* utilizando una función llamada **entropía** que tiene este aspecto (gráfico). La función de entropía se denota convencionalmente como $H$ de este número $p_1$ ($H(p_1)$) y la función se parece a esta curva de aquí donde el eje horizontal es $p_1$, la fracción de gatos en la muestra, y el eje vertical es el valor de la entropía. \n",
    "\n",
    "Ejemplo 1: donde $p_1$ es 3/6 = 0,5, el valor de la $H(p_1)$ = 1. \n",
    "\n",
    "Te das cuenta de que esta curva es:\n",
    "- más alta cuando tu conjunto de ejemplos es 50-50, por lo que es más impuro como una impureza de 1 o con una entropía de 1 cuando tu conjunto de ejemplos es 50-50, \n",
    "- mientras que en contraste si tu conjunto de ejemplos era o todos gatos o todos no gatos entonces la entropía es cero. \n",
    "\n",
    "Repasemos algunos ejemplos más para intuir mejor la entropía y su funcionamiento. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/08_LI.jpg\"   style=\"width:80%;\" >\n",
    "</figure>\n",
    "\n",
    "Ejemplo 2: hay un conjunto diferente de ejemplos con cinco gatos y un perro, por lo que $p_1$ la fracción de ejemplos positivos, una fracción de ejemplos etiquetados como 1 es 5/6=0,83. en el gráfico, el valor de $p_1$ se corresponde con que la entropía es de alrededor de 0,65. \n",
    "\n",
    "Ejemplo 3: esta muestra de seis imágenes tiene todos los gatos por lo que $p_1$ es 6/6 porque todos los seis son gatos y la entropía de $p_1$ es este punto de aquí que es 0. \n",
    "\n",
    "Vemos que a medida que se pasa de 3/6 a seis de seis gatos, la impureza disminuye de 1 a 0 o, en otras palabras, *la pureza aumenta a medida que se pasa de una mezcla 50-50 de gatos y perros a sólo gatos*.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/09_LI.jpg\"   style=\"width:80%;\" >\n",
    "</figure>\n",
    "\n",
    "Ejemplo 4: aquí tenemos otra muestra con dos gatos y cuatro perros, por lo que $p_1$ es 2/6=1/3=0.33, y si leemos la entropía resulta ser aproximadamente 0,92. Esto es realmente bastante impuro y, en particular, este conjunto es más impuro que el ejemplo 2 porque está más cerca de una mezcla 50-50, por lo que la impureza aquí es de 0,92 en comparación con 0,65. \n",
    "\n",
    "Ejemplo 5: si tenemos un conjunto de todos los seis perros entonces $p_1$=0 y la entropía $H(p_1)$=0 por lo que hay cero impureza o esto sería un conjunto completamente puro de todos no gatos o todos  perros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0353ed-3b50-49ba-adbb-9958fbdca1b4",
   "metadata": {},
   "source": [
    "## ¿Cómo es la ecuación de la entropia?\n",
    "\n",
    "Recordemos que $p_1$ es la fracción de ejemplos que son iguales a los ejemplos de etiqueta 1 (gatos en nuestro ejemplo), así que, si tienes una muestra que tiene 2/3 de gatos, entonces esa muestra debe tener 1/3 de no gatos. Definiré $p_0$ como la fracción de ejemplos de etiqueta 0 (no gatos en nuestro ejemplo), esto es 1-$p_1$. \n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/10.png\"   style=\"width:80%;\" >\n",
    "</figure>\n",
    "\n",
    "La función de entropía se define entonces como $H(p_1) = -p_1log_2(p_1) - p_0log_2(p_0)$ , y por convención al calcular la entropía tomamos los logaritmos en base 2 en lugar de en base e. Alternativamente, esto también es igual a $H(p_1) = -p_1log_2(p_1) - (1-p_1)log_2(1-p_1)$\n",
    "\n",
    "Si se traza esta función se verá que será exactamente esta gráfica a la izquierda.\n",
    "\n",
    ">*Nota: si $p_1$ o $p_0$ es igual a 0 entonces una expresión como esta se verá como “$0 log(0)$”, y $log(0)$ es técnicamente indefinido, es en realidad el infinito negativo. Pero por convención para los propósitos de calcular la entropía, tomaremos $0 log(0)$ para ser igual a 0 y eso calculará correctamente la entropía como 0 o como 1 para ser igual a 0.*\n",
    "\n",
    "Si estás pensando que esta definición de entropía se parece un poco a la definición de la pérdida logística que aprendimos anteriormente, en realidad hay una razón matemática de por qué estas dos fórmulas se parecen tanto. Pero no tienes que preocuparte por ello y no entraremos en ello en esta clase. Pero la aplicación de esta fórmula para la entropía debería funcionar bien cuando se construye un árbol de decisión. \n",
    "\n",
    "> Hay otras funciones que se parecen a ésta, van de cero a uno y luego vuelven a bajar. Por ejemplo, si buscas en paquetes de código abierto, también puedes oír hablar de algo llamado el **criterio de Gini**, que es otra función que se parece mucho a la función de entropía, y que también funcionará bien para construir árboles de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850dcce-c8f6-4c20-b2e7-ef48c3757b38",
   "metadata": {},
   "source": [
    "## ¿Que es Information Gain (Ganancia de Información)\n",
    "\n",
    "Al construir un árbol de decisión, la forma en que decidiremos qué característica dividir en un nodo se basará en la elección de la característica que reduce más la entropía (reduce la impureza, o maximiza la pureza). En el aprendizaje de árboles de decisión, la reducción de la entropía se llama *Information Gain* (o ganancia de información).\n",
    "\n",
    "Veamos un ejemplo:\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/11.png\"   style=\"width:80%;\" >\n",
    "</figure>\n",
    "\n",
    "Tenemos 5 gatos y 5 perros para dividir\n",
    "\n",
    "Tenemos 3 ejemplos de separación en función de 3 caratcterísticas. ¿Cuál de estas divisiones es mejor?\n",
    "\n",
    "- En el ejemplo de la izquierda: dividimos por la característica de la forma de la oreja en el nodo raíz, obtenido cinco ejemplos a la izquierda y cinco a la derecha. A la izquierda, tendríamos 4/5 gatos, por lo que $p_1$=4/5=0,8. En la derecha, 1/5 son gatos, por lo que $p_1$=1/5=0,2. Si aplicas la fórmula de la entropía en cada subconjunto de datos de la izquierda y de la derecha, encontramos que el **grado de impureza**. A la izquierda es la entropía $H(0,8)$=0,72, y en la derecha, la entropía $H(0,2)$=0,72.\n",
    "- En el ejemplo del medio: divide por la característica de la forma de la cara. Así, a la izquierda tenemos 4/7 ejemplos serían gatos, por lo que $p_1$=4/7 y a la derecha, 1/3 son gatos, por lo que $p_1$=1/3. La entropía de $H(4/7)$=0,99 y la $H(1/3)$=0,92. Por tanto, el grado de impureza de los nodos de la izquierda y la derecha parece mucho mayor, 0,99 y 0,92 en comparación con 0,72 y 0,72. \n",
    "- En el ejemplo de la derecha, usa la característica de los bigotes en el nodo raíz. En este caso, $p_1$ a la izquierda es 3/4, $p_1$ a la derecha es 2/6, y los valores de entropía son $H(0,75)$=0,81 a la izquierda y la entropía $H(0,33)$=0,92 a la derecha. \n",
    "\n",
    "La pregunta clave que tenemos que responder es, dadas estas tres opciones de una característica para usar en el nodo raíz, ¿cuál creemos que funciona mejor? \n",
    "\n",
    "Resulta que en lugar de mirar estos números de entropía y compararlos, sería útil tomar una media ponderada de ellos. La decisión clave es, de estas tres posibles opciones de características a utilizar en el nodo raíz, ¿cuál queremos utilizar? \n",
    "\n",
    "Asociados a cada una de estas divisiones hay dos números, la entropía de la subrama izquierda y la entropía de la subrama derecha. Para elegir entre estos, nos gusta combinar estos dos números en un solo número. La forma en que vamos a combinar estos dos números es tomando un promedio ponderado. Porque la importancia de tener una baja entropía en, digamos, la sub-rama de la izquierda o de la derecha, también depende de cuántos ejemplos entraron en la sub-rama de la izquierda o de la derecha.\n",
    "\n",
    "- En el ejemplo de la izquierda tenemos, 5/10 ejemplos fueron a la sub-rama de la izquierda, por lo que podemos calcular la media ponderada como 5/10$H(0,8)$, y luego añadir a que 5/10 ejemplos también fueron a la sub-rama de la derecha, 5/10$H(0,2)$. \n",
    "- En el ejemplo del medio, la sub-rama izquierda había recibido 1/10 ejemplos. Así vamos a calcular 7/10$H(0,57) más, la sub-rama derecha tenía tres de 10 ejemplos, así que más 3/10$H(0,3)$. \n",
    "- En el ejemplo de la derecha, calcularemos 4/10 $H(0,75) + 6/10$H(0,33)$.\n",
    "\n",
    "Ahora para calcular la reducción de la entropía en comparación con si no hubiéramos dividido en absoluto, tenemos que ir al nodo raíz, recuerde que el nodo raíz que tenemos comenzó con los 10 ejemplos (con cinco gatos y perros). Así en el nodo raíz, tuvimos $p_1$=5/10=0,5. La entropía de los nodos de la raíz es igual a 1. Esto fue máximo en la pureza porque era cinco gatos y cinco perros. \n",
    "\n",
    "La fórmula que en realidad que vamos a utilizar para elegir una división va a ser la entropía en el nodo raíz menos la suma de las relaciones de ejemplos por su entropia de cada rama.\n",
    "\n",
    "- En el ejemplo de la izquierda: el resultado final resulta ser 0,28. \n",
    "- En el ejemplo del centro: resulta ser 0,03. \n",
    "- En el ejemplo de la izquierda: resulta ser 0,12. \n",
    "\n",
    "Estos números que acabamos de calcular, 0,28, 0,03, y 0,12, se llaman la **ganancia de información**, y lo que mide es la *reducción de la entropía que se obtiene en el árbol resultante de hacer una división*. Porque la entropía era originalmente 1 en el nodo raíz y al hacer la división, se termina con un valor más bajo de la entropía y la diferencia entre esos dos valores es una reducción de la entropía, y eso es 0,28 en el caso de la división en la forma de la oreja. \n",
    "\n",
    "## ¿Por qué nos molestamos en calcular la reducción de entropía en lugar de sólo la entropía en las sub-ramas izquierda y derecha? \n",
    "\n",
    "Resulta que uno de los criterios de parada para decidir cuándo no molestarse en seguir dividiendo es si la reducción de entropía es demasiado pequeña. En este caso, se podría decidir que se está aumentando el tamaño del árbol innecesariamente y que se corre el riesgo de sobreajuste al dividirlo, y decidir no molestarse si la reducción de la entropía es demasiado pequeña o está por debajo de un umbral. \n",
    "\n",
    "En el ejemplo de la izquierda, la división en la forma de la oreja resulta en la mayor reducción de entropía, 0,28 es mayor que 0,03 o 0,12 y por lo tanto elegiríamos dividir en la característica de la forma de la oreja en el nodo raíz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f9bd4-87f5-4473-94b9-96f2b678662c",
   "metadata": {},
   "source": [
    "## Notación formal\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/12.png\"   style=\"width:80%;\" >\n",
    "</figure>\n",
    "\n",
    "- $w^{left}$ y $w^{right}$ son respectivamente la fracción de ejemplos que fueron a la rama izquierda, y la fracción de ejemplos que fueron a la rama derecha.\n",
    "- $p_1^{left}$ y $p_1^{right}$ es respectivamente la fracción de ejemplos en el subárbol izquierdo y derecho que tiene etiquetas positivas \n",
    "- $p_1^{root}$ es la fracción de ejemplos que son positivos en el nodo raíz.\n",
    "- La **ganancia de información** se define entonces como $H(p_1^{root}) - (w^{left}H(p_1^{left}) + w^{right}H(p_1^{right}))$ \n",
    "\n",
    "Con esta definición de entropía se puede calcular la ganancia de información asociada a la elección de cualquier característica particular para dividir en el nodo. Entonces, de todos los posibles futuros, que podría elegir para dividir, usted puede entonces elegir el que le da la mayor ganancia de información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d92507-6b61-4ef8-9d99-887f6304f950",
   "metadata": {},
   "source": [
    "## Resumen del proceso general de construcción de un árbol de decisión\n",
    "\n",
    "1. Comienza con todos los ejemplos de entrenamiento en el nodo raíz del árbol \n",
    "2. Calcula la ganancia de información para todas las características posibles y elige la característica para dividir, que da la mayor ganancia de información. \n",
    "3. Una vez elegida esta característica, se divide el conjunto de datos en dos subconjuntos según la característica seleccionada, se crean ramas izquierda y derecha del árbol y se envían los ejemplos de entrenamiento a la rama izquierda o a la derecha, dependiendo del valor de esa característica para ese ejemplo. Esto le permite haber realizado una división en el nodo raíz.\n",
    "4. Seguirá repitiendo el proceso de división en la ramas izquierda y derecha del árbol y así sucesivamente. Siga haciéndolo hasta que se cumpla uno o varios de los criterios de parada. Los criterios de parada pueden ser: \n",
    "   - Cuando un nodo es 100% una sola cláusula, alguien ha alcanzado la entropía de cero. \n",
    "   - Cuando la división adicional de un nodo haga que el árbol exceda la profundidad máxima que usted había establecido.\n",
    "   - Si la ganancia de información de una división adicional es menor que un determinado umbral. \n",
    "   - Si el número de ejemplos en un nodo es inferior a un umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1f689-6c4a-4464-aa17-2c06dc3def0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "664eebf5-59af-4d6e-869f-52cd1a904290",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
