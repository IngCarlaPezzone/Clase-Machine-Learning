{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e1ba74-03b6-432d-8fa5-fb0ced58e035",
   "metadata": {},
   "source": [
    "# Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427c124-b5a9-4983-bf05-edad02ed286e",
   "metadata": {},
   "source": [
    "## Ejemplo de clasificación\n",
    "\n",
    "Para explicar cómo funcionan los árboles de decisión, vamos a utilizar un ejemplo de clasificación de gatos. Estás dirigiendo un centro de adopción de gatos y, dadas unas cuantas características, quieres entrenar un clasificador que te diga rápidamente si un animal es un gato o no. Tengo aquí 10 ejemplos de entrenamiento. Asociado con cada uno de estos 10 ejemplos, vamos a tener características relativas a la forma de las orejas del animal, la forma de la cara, si tiene bigotes, y luego la etiqueta de verdad que desea predecir este gato animal. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/01.png\"   style=\"width:70%;\" >\n",
    "</figure>\n",
    "\n",
    "Este conjunto de datos contiene cinco gatos y cinco perros. Las **características de entrada x** son estas tres columnas, y la salida **objetivo** que se quiere predecir, **y**, es esta columna final de, ¿es un gato o no? \n",
    "\n",
    "En este ejemplo, las características x toman valores *categóricos*. En otras palabras, las características toman sólo unos pocos valores discretos: Sus formas son puntiagudas o blandas. La forma de la cara es redonda o no redonda y los bigotes están presentes o ausentes. \n",
    "\n",
    "Se trata de una tarea de clasificación binaria porque las etiquetas también son uno o cero. Por ahora, cada una de las características $x_1$, $x_2$ y $x_3$ toma sólo dos valores posibles. Más adelante hablaremos de las características que pueden tener más de dos valores posibles, así como de las características de valor continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b8413-14d7-4eef-ad0e-68c678ed4edd",
   "metadata": {},
   "source": [
    "## ¿Qué es un árbol de decisión?\n",
    "\n",
    "Este es un ejemplo de un modelo que puede obtenerse después de entrenar un algoritmo de aprendizaje de árbol de decisión en el conjunto de datos que acabamos de ver. \n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/02.png\"   style=\"width:100%;\" >\n",
    "</figure>\n",
    "\n",
    "- Cada uno de los óvalos o rectángulos se llama **nodo** en el árbol. \n",
    " - El nodo superior del árbol, se llama el **nodo raíz** del árbol.\n",
    " - Los nodos ovalados son **nodos de decisión** porque miran una característica particular y luego, basados en el valor de la característica, hacen que usted decida si ir a la izquierda o a la derecha en el árbol.\n",
    " - Los nodos rectangulares en la parte inferior, se llaman **nodos hoja**. Son los que hacen una predicción.\n",
    "\n",
    "La forma en que este modelo funciona es:\n",
    "\n",
    "- Tiene un nuevo ejemplo de prueba, tiene un gato donde la forma de la oreja tiene puntiaguda, la forma de la cara es redonda, y los bigotes están presentes. \n",
    "- La forma en que este modelo mirará este ejemplo y tomará una decisión de clasificación es que empezará con este ejemplo en este nodo superior del árbol (nodo raiz), y miraremos la característica escrita dentro, que es la *forma de la oreja*. \n",
    "- Basándonos en el valor de la forma de la oreja de este ejemplo iremos a la izquierda o a la derecha. El valor de la forma de la oreja de este ejemplo es puntiaguda, por lo que bajaremos por la rama izquierda del árbol, y terminaremos en este nodo ovalado siguiente. \n",
    "- Luego miramos la forma de la cara de este ejemplo, que resulta ser redonda, y entonces seguiremos esta flecha hacia abajo (nodo hoja). El algoritmo hará una inferencia que piensa que esto es un gato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd6518-b68f-4498-b18a-877a05b89e51",
   "metadata": {},
   "source": [
    "## Proceso de construcción\n",
    "\n",
    "El proceso de construcción de un árbol de decisión dado un conjunto de entrenamiento tiene unos cuantos pasos. Veamos el proceso general de lo que hay que hacer para construir un árbol de decisión, dado un conjunto de entrenamiento de 10 ejemplos de gatos y perro.\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/03.png\"   style=\"width:100%;\" >\n",
    "</figure>\n",
    "\n",
    "1. Decidir qué característica utilizar en el nodo raíz. Digamos que decidimos elegir como característica la forma de la oreja. Lo que significa es que vamos a decidir mirar todos nuestros ejemplos de entrenamiento y dividirlos según el valor de la característica de la *forma de la oreja*. \n",
    "En particular, vamos a elegir los cinco ejemplos con orejas puntiagudas y moverlos hacia abajo a la izquierda. Elijamos los cinco ejemplos con orejas caídas y desplacémoslos hacia la derecha. \n",
    "\n",
    "2. Veamos la **rama izquierda** del árbol de decisión para decidir qué nodos poner allí. En particular, qué característica queremos utilizar a continuación. Digamos que usted decide utilizar la característica de la *forma de la cara* allí. Lo que haremos ahora es tomar los cinco ejemplos del punto 1 y dividirlos en dos subconjuntos basados en su valor de la forma de la cara. Tomaremos los cuatro ejemplos de estos cinco con una forma de *cara redonda* y los moveremos hacia la izquierda. El único ejemplo con una forma de *cara no redonda* y lo moveremos hacia abajo a la derecha. \n",
    "Por último, observamos que estos cuatro ejemplos son todos gatos cuatro de ellos son gatos. En lugar de dividir más, se creó un nodo de hoja que hace una predicción. \n",
    "\n",
    "3. Repetimos un proceso similar en la **rama derecha** de este árbol de decisión. Centramos la atención en los cinco ejemplos que mencionamos en el punto 1. Tendríamos que elegir alguna característica aquí para utilizar la división de estos cinco ejemplos más, por ejemplo, la característica de los *bigotes*, entonces dividiríamos estos cinco ejemplos basados en donde los bigotes están presentes o ausentes.\n",
    "\n",
    "Notarás que uno de cada uno de los ejemplos de la izquierda son gatos y cero de cada cuatro son gatos. Cada uno de estos nodos es completamente puro, es decir, todos gatos o no gatos y ya no hay una mezcla de gatos y perros. Podemos crear estos nodos hoja, haciendo una predicción de gato a la izquierda y una predicción de no gato aquí a la derecha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303da75-18a6-48fa-b1f8-7263bc10d65c",
   "metadata": {},
   "source": [
    "## Decisiones claves durante el proceso de construcción\n",
    "\n",
    "1. **¿Cómo elegir qué características utilizar para dividir en cada nodo?** \n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./Imagenes/04.png\"   style=\"width:100%;\" >\n",
    "</figure>\n",
    "\n",
    "En el nodo raíz, así como en la rama izquierda y en la rama derecha del árbol de decisión, teníamos que decidir si había algunos ejemplos en ese nodo que comprendieran una mezcla de gatos y perros. Para elegir en qué característica dividir, los árboles de decisión intentan *maximizar la pureza* (o *minimizar la impureza*), es decir, llegar a los subconjuntos, que son lo más cercano posible a todos los gatos o todos los perros. Cuando los subconjuntos son así, se dicen que son subconjuntos *puros*, lo que significa que sólo hay una clase, o bien sólo gatos o bien no sólo gatos en ambas sub-ramas.\n",
    "    \n",
    "El algoritmo de aprendizaje del árbol de decisión tiene que elegir entre las distintas características ¿cuál es la que da lugar a la mayor pureza de las etiquetas en las sub-ramas izquierda y derecha? Porque si puedes llegar a un subconjunto de ejemplos altamente puro, entonces puedes predecir gato o no gato y acertar en su mayoría.\n",
    "\n",
    "2. **¿Cuándo se deja de dividir?**\n",
    "\n",
    "- El criterio que utilizamos ahora fue hasta que sé que hay 100%, todos los gatos o un 100% de los perros (no los gatos). Porque en ese punto parece natural construir un nodo hoja que sólo hace una predicción de clasificación. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/05.png\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "- Alternativamente, también podría decidir dejar de dividir cuando la división no supere la *profundidad máxima*. La profundidad de un nodo es un parámetro que se define como el número de saltos que se necesita para llegar desde el nodo raíz (que se denota la parte superior) a ese nodo en particular. Así que el nodo raíz tarda cero profundidad. Los nodos por debajo de él están en la profundidad 1 y en las de abajo estaría en la Profundidad 2. \n",
    "\n",
    "  Si usted hubiera decidido que la profundidad máxima del árbol de decisión es digamos 2, entonces decidiría no dividir ningún nodo por debajo de este nivel para que el árbol nunca llegue a la Profundidad 3. \n",
    "\n",
    "  Una de las razones por las que podría querer limitar la profundidad del árbol de decisión es:\n",
    "\n",
    "   - para asegurarse de que para nosotros el árbol no se hace demasiado grande y difícil de manejar\n",
    "   - para  mantener el árbol pequeño, lo hace menos propenso al sobreajuste. \n",
    "\n",
    "- Otro criterio que podría utilizar para decidir dejar de dividir podría ser cuando mejora en la *puntuación de prioridad*, es decir, de cuando se encuentra por debajo de un determinado umbral. Esto es, si al dividir un nodo se obtienen mejoras mínimas en la pureza o disminuye la impureza. Pero si las ganancias son demasiado pequeñas, podrías no molestarte. \n",
    "  \n",
    "  De nuevo, esto es tanto para mantener los árboles más pequeños como para reducir el riesgo de sobreajuste.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./Imagenes/06.png\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "- Por último, si el *número de ejemplos* de un nodo está por debajo de un determinado umbral, entonces también podría decidir dejar de dividir. Por ejemplo, si en el nodo raíz hemos dividido en la característica de la forma de la cara, entonces la rama derecha habrá tenido sólo tres ejemplos de entrenamiento con un gato y dos perros y en lugar de dividir esto en subconjuntos aún más pequeños, si usted decidió no dividir más, entonces usted acaba de crear un nodo de decisión y porque esto hace una predicción de no gato. \n",
    "\n",
    "  De nuevo, una razón por la que podrías decidir que no vale la pena dividir esto es para mantener el árbol más pequeño y evitar el sobreajuste.\n",
    "\n",
    "\n",
    "> Andrew Ng dice: *Cuando miro las tareas de aprendizaje de los árboles de decisión, a veces me siento como chico, hay un montón de piezas diferentes y un montón de cosas diferentes en este algoritmo.\n",
    "> Parte de la razón por la que podría sentirse así está en la evolución de los árboles de decisión. Hubo un investigador que propuso una versión básica de los árboles de decisión y luego un investigador diferente dijo, oh, podemos modificar esta cosa de esta manera, como su nuevo criterio de división. Luego, otro investigador propuso una cosa diferente como, oh, tal vez deberíamos dejar de sudar cuando se alcanza una cierta profundidad máxima. A lo largo de los años, diferentes investigadores vinieron con diferentes refinamientos del algoritmo.\n",
    "> Como resultado de ello, funciona muy bien, pero cuando nos fijamos en todos los detalles de cómo implementar un árbol de decisión, se siente un montón de piezas diferentes como ¿por qué hay tantas maneras diferentes de decidir cuándo dejar de dividir?. \n",
    "> Si a usted le parece algo complicado y desordenado, a mí también me lo parece. Pero estas piezas diferentes, encajan en un algoritmo de aprendizaje muy eficaz y lo que se tiene que aprender es la clave, las ideas más importantes para que funcione bien.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e7ffa-3459-4690-bd1b-5a1c36494c26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b0353ed-3b50-49ba-adbb-9958fbdca1b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0850dcce-c8f6-4c20-b2e7-ef48c3757b38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
